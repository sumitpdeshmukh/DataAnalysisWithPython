{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Analysis of NYT data\n",
    "- Used NYT API to collect NYT data\n",
    "- Link to NYT developer docs : [NYT API Documentation](http://developer.nytimes.com/)\n",
    "- You need an API key from NYT that is kept as a environment variable before starting this notebook\n",
    "- API key is needed only to download the NYT data\n",
    "- Analysis is done on downloaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect the data\n",
    "- Downloading the articles published on NYT from 1852 to 2016 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Request to get population data for cities based on the city data gathered\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import sys \n",
    "import operator \n",
    "from datetime import datetime, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "api_key = \"api-key=\" + os.getenv(\"nyt_archive_key\")\n",
    "current_dir = os.getcwd() # Gets the current working directory of this file\n",
    "download_dir = current_dir + \"/data/NYT/archive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A sample format of NYT article json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random sample of data in json format: \n",
      "{\n",
      "    \"_id\": \"4fbfd31a45c1498b0d0094f5\",\n",
      "    \"abstract\": null,\n",
      "    \"blog\": [],\n",
      "    \"byline\": null,\n",
      "    \"document_type\": \"article\",\n",
      "    \"headline\": {\n",
      "        \"kicker\": \"1\",\n",
      "        \"main\": \"France and Great Britain.\"\n",
      "    },\n",
      "    \"keywords\": [],\n",
      "    \"lead_paragraph\": \"LOUIS NAPOLEON just now seems omnipotent in France. He has the army on his side, at his bidding. That vast body of armed men perpetrates the grossest outrages upon law, and tramples alike upon personal and public rights.\",\n",
      "    \"multimedia\": [],\n",
      "    \"news_desk\": null,\n",
      "    \"print_page\": \"4\",\n",
      "    \"pub_date\": \"1852-01-01T00:03:58Z\",\n",
      "    \"section_name\": null,\n",
      "    \"slideshow_credits\": null,\n",
      "    \"snippet\": \"LOUIS NAPOLEON just now seems omnipotent in France. He has the army on his side, at his bidding. That vast body of armed men perpetrates the grossest outrages upon law, and tramples alike upon personal and public rights....\",\n",
      "    \"source\": \"The New York Times\",\n",
      "    \"subsection_name\": null,\n",
      "    \"type_of_material\": \"Article\",\n",
      "    \"web_url\": \"http://query.nytimes.com/gst/abstract.html?res=9C02E2DC1331E234BC4953DFB7668389649FDE\",\n",
      "    \"word_count\": 756\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "nytdata = requests.get(\"https://api.nytimes.com/svc/archive/v1/1852/1.json?\" + api_key).json()\n",
    "print(\"A random sample of data in json format: \")\n",
    "print(json.dumps(nytdata[\"response\"][\"docs\"][1], indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Store data\n",
    "- Stored data in `~/year/month/*` format\n",
    "- Do not indent and sort the keys to save on disk space and time to write to disk\n",
    "- There is a rate limit for downloading the data. On one day you can atmost make 2000 request with one API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createdir(path): # Function to create directory if it does not exist already\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        return True\n",
    "    except OSError as exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pass from and to values or else function will download all data since 1852\n",
    "def downloadNYTdata(datatype, fromyear=1852, toyear=2016) :\n",
    "    for year in range(fromyear,toyear + 1):\n",
    "        for month in range(1,13):\n",
    "            year = str(year)\n",
    "            month = str(month)\n",
    "            resp = requests.get(\"https://api.nytimes.com/svc/\" + datatype + \"/v1/\" + year + \"/\" + month + \".json?\" + api_key)\n",
    "            if resp.status_code is 200:\n",
    "                outputdir = download_dir + year + \"/\" + month\n",
    "                createdir(outputdir)\n",
    "                try:\n",
    "                    with open(outputdir + \"/\" + year + \"_\" + month + \".json\", \"w\") as jsonfile:\n",
    "                        json.dump(resp.json(), jsonfile)\n",
    "                except:\n",
    "                    print(\"hits: \" + str(data_dict[\"response\"][\"meta\"][\"hits\"]) + \" and x: \" + str(x))\n",
    "            else:\n",
    "                print(\"No data for year:\" + year + \"and Month:\" + month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keep in mind the number of requests you are making\n",
    "- Call following function to start dowloading data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# downloadNYTdata(datatype='archive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Article schema and helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a typical article lets define an article schema\n",
    "class Article:\n",
    "    def __init__(self):\n",
    "        self.headline = \"\"\n",
    "        self.author = \"\"\n",
    "        self.weburl = \"\"\n",
    "        self.sectname = \"\"\n",
    "        self.typeofart = \"\"\n",
    "        self.pubdate = datetime.today()\n",
    "        self.keywords = []\n",
    "    def __repr__(self):\n",
    "        return repr((self.headline, self.author, self.weburl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to create object of Article class form json data     \n",
    "def getArtObj(json_data):\n",
    "    article = Article()\n",
    "           \n",
    "    article.weburl = json_data[\"web_url\"]\n",
    "    article.sectname = json_data[\"section_name\"]\n",
    "    article.typeofart = json_data[\"type_of_material\"]\n",
    "    article.pubdate = datetime.strptime(json_data[\"pub_date\"][0:19], '%Y-%m-%dT%H:%M:%S')\n",
    "    article.keywords = [word for word in json_data[\"keywords\"]]\n",
    "    \n",
    "    if json_data[\"headline\"] is not None:\n",
    "        if \"main\" in json_data[\"headline\"] and json_data[\"headline\"][\"main\"]:\n",
    "            article.headline = json_data[\"headline\"][\"main\"]\n",
    "        \n",
    "    if json_data[\"byline\"] is not None:\n",
    "        if \"original\" in json_data[\"byline\"] and json_data[\"byline\"][\"original\"]:\n",
    "            article.author = json_data[\"byline\"][\"original\"][3:]\n",
    "    \n",
    "    return article\n",
    "\n",
    "AllArticles = {} # Dictionary to hold all articles objects\n",
    "\n",
    "# Function to create Data Structure of All articles\n",
    "def createDS(data_dict):\n",
    "    for x in range(0, len(data_dict[\"response\"][\"docs\"])):\n",
    "        try:\n",
    "            json_data = data_dict[\"response\"][\"docs\"][x]\n",
    "            AllArticles[json_data[\"_id\"]] = getArtObj(json_data)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "# Function to load Data in memory\n",
    "def loadArticlesData(fromyear= 1852, toyear = 2017):\n",
    "    for year in range(fromyear,toyear):\n",
    "        for month in range(1,13):\n",
    "            year = str(year)\n",
    "            month = str(month)\n",
    "            if os.path.exists(download_dir + year + '/'+ month + '/'):\n",
    "                json_files = glob.glob(download_dir + year + '/'+ month + '/*.json')\n",
    "                if json_files is not None:\n",
    "                    with open(json_files[0]) as datafile:\n",
    "                        data_dict = json.load(datafile)\n",
    "                        createDS(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the data and find answers to some trivial questions\n",
    "#### Analysis - 1\n",
    "- Total number of articles\n",
    "- What are different News Categories?\n",
    "- Number of articles NYT publishes per year?\n",
    "- Which author published the most articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles:  582131\n"
     ]
    }
   ],
   "source": [
    "loadArticlesData(2011, 2017)\n",
    "print(\"Total number of articles: \", len(AllArticles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author_dict = {} #{'name': {'Num Articles': <num> , 'Publish Date' : ['', '', '']}}\n",
    "articletypes = {} #{'<type>' : 'count'}\n",
    "YrArtilces = {}\n",
    "for key, article in AllArticles.items():\n",
    "    count = articletypes.get(article.typeofart)\n",
    "    if count is None:\n",
    "        articletypes[article.typeofart] = 1\n",
    "    else:\n",
    "        articletypes[article.typeofart] = count + 1\n",
    "    \n",
    "    count = YrArtilces.get(article.pubdate.year)\n",
    "    if count is None:\n",
    "        YrArtilces[article.pubdate.year] = 1\n",
    "    else:\n",
    "        YrArtilces[article.pubdate.year] = count + 1\n",
    "\n",
    "    if article.author:    \n",
    "        details = author_dict.get(article.author)\n",
    "        if details is None:\n",
    "            author_dict[article.author] = {\"Num Articles\" : 1, \"Publish Date\" : [article.pubdate]}\n",
    "        else:\n",
    "            count = details.get(\"Num Articles\") \n",
    "            details[\"Num Articles\"] = count + 1\n",
    "\n",
    "            publist = details.get(\"Publish Date\")\n",
    "            publist.append(article.pubdate)\n",
    "            details[\"Publish Date\"] = publist\n",
    "\n",
    "            author_dict[article.author] = details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Article Types are\n",
      "('News', 230867)\n",
      "('Blog', 181273)\n",
      "('Review', 27175)\n",
      "('Brief', 24994)\n",
      "('Slideshow', 19328)\n",
      "('Video', 18959)\n",
      "('Op-Ed', 15420)\n",
      "('Letter', 12061)\n",
      "('Interactive Feature', 10947)\n",
      "('Editorial', 8023)\n",
      "('Obituary', 6067)\n",
      "('Schedule', 6034)\n",
      "('Paid Death Notice', 4777)\n",
      "('Question', 3542)\n",
      "('List', 3250)\n",
      "('An Analysis; News Analysis', 1506)\n",
      "('Quote', 1375)\n",
      "('briefing', 945)\n",
      "('Recipe', 932)\n",
      "('Correction', 728)\n",
      "\n",
      "Top 20 Reporters according to number of published articles\n",
      "THE ASSOCIATED PRESS --- 27456\n",
      "THE NEW YORK TIMES --- 16301\n",
      "PAUL KRUGMAN --- 5388\n",
      "REUTERS --- 5002\n",
      "THE EDITORIAL BOARD --- 4339\n",
      "THE LEARNING NETWORK --- 2766\n",
      "DEB AMLEN --- 2692\n",
      "MICHAEL J. DE LA MERCED --- 2407\n",
      "DEALBOOK --- 2364\n",
      "MICHAEL D. SHEAR --- 1942\n",
      "ANN CARRNS --- 1803\n",
      "FLORENCE FABRICANT --- 1776\n",
      "DAVE ITZKOFF --- 1747\n",
      "DAVID WALDSTEIN --- 1727\n",
      "INTERNATIONAL HERALD TRIBUNE --- 1725\n",
      "KATHERINE SCHULTEN --- 1675\n",
      "ANDREW C. REVKIN --- 1662\n",
      "TYLER KEPNER --- 1575\n",
      "KATHRYN SHATTUCK --- 1562\n",
      "ANDREW ROSENTHAL --- 1548\n"
     ]
    }
   ],
   "source": [
    "sorted_tuple = sorted(articletypes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Top 20 Article Types are\")\n",
    "print(*sorted_tuple[:20], sep=\"\\n\")\n",
    "print(\"\", sep=\"\\n\\n\")\n",
    "sorted_tuple = sorted(author_dict.items(), key=lambda x: x[1][\"Num Articles\"], reverse=True)\n",
    "print(\"Top 20 Reporters according to number of published articles\")\n",
    "for t in sorted_tuple[0:20]: print(t[0] + \" --- \" + str(t[1][\"Num Articles\"]), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles published per year\n"
     ]
    }
   ],
   "source": [
    "# Per year number of articles\n",
    "print(\"Articles published per year\")\n",
    "sorted_tuple = sorted(YrArtilces.items(), key=operator.itemgetter(0))\n",
    "\n",
    "years = []\n",
    "numarts = []\n",
    "for t in sorted_tuple:\n",
    "    years.append(t[0])\n",
    "    numarts.append(t[1])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "width=0.35\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar displaying its height\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                '%d' % int(height),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "ax.set_ylabel('Number of Published Articles')\n",
    "ax.set_xlabel('Years')\n",
    "ax.set_title('Years vs Number of Published Articles')\n",
    "ax.set_xticks(years)\n",
    "ax.get_xaxis().get_major_formatter().set_useOffset(False)\n",
    "rects = ax.bar(years, numarts, width)\n",
    "autolabel(rects)\n",
    "\n",
    "#plt.show()\n",
    "#fig.set_size_inches(20, 10.5)\n",
    "plt.savefig(\"Q2img/YearArticlePlot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Articles published per year](Q2img/YearArticlePlot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reporters on President Donald Trump\n",
    "#### Analysis - 2\n",
    "\n",
    "- I am interested in finding reporters reporting on President Donald Trump and store that count\n",
    "- Recently President Trump accused New york times reporters on spreading lies about him.\n",
    "- I want to know exactly which reporters reported on him the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Reporters reporting on President Donald Trump and his family are\n",
      "ALAN RAPPEPORT --- 308\n",
      "MAGGIE HABERMAN --- 292\n",
      "THE EDITORIAL BOARD --- 205\n",
      "NICK CORASANITI --- 130\n",
      "THE NEW YORK TIMES --- 104\n",
      "ASHLEY PARKER --- 92\n",
      "NATE COHN --- 89\n",
      "ROSS DOUTHAT --- 85\n",
      "TRIP GABRIEL --- 85\n",
      "FRANK BRUNI --- 84\n",
      "JEREMY W. PETERS --- 77\n",
      "GAIL COLLINS --- 75\n",
      "PAUL KRUGMAN --- 72\n",
      "JONATHAN MARTIN --- 66\n",
      "MICHAEL BARBARO --- 64\n",
      "CHARLES M. BLOW --- 61\n",
      "MATT FLEGENHEIMER --- 61\n",
      "ALEXANDER BURNS --- 56\n",
      "ANDREW ROSENTHAL --- 55\n",
      "AMY CHOZICK --- 52\n",
      "The number of urls we are interested in are:  2113\n"
     ]
    }
   ],
   "source": [
    "\n",
    "author_dict = {} #{'reporter name': {'weburls' : [list of weburls for these articles], 'reports': count} }\n",
    "urllist = []\n",
    "for key, article in AllArticles.items():\n",
    "    if article.author:\n",
    "        for data in article.keywords:\n",
    "            if 'persons' in data['name'] and 'trump' in data['value'].lower():\n",
    "                details = author_dict.get(article.author) \n",
    "                    \n",
    "                if details is None:\n",
    "                    author_dict[article.author] = {\"reports\" : 1, \"weburls\" : [article.weburl]}\n",
    "                else:\n",
    "                    count = details.get(\"reports\") \n",
    "                    details[\"reports\"] = count + 1\n",
    "\n",
    "                    urls = details.get(\"weburls\")\n",
    "                    urls.append(article.weburl)\n",
    "                    details[\"weburls\"] = urls\n",
    "\n",
    "                    author_dict[article.author] = details\n",
    "\n",
    "sorted_tuple = sorted(author_dict.items(), key=lambda x: x[1][\"reports\"], reverse=True)\n",
    "print(\"Top Reporters reporting on President Donald Trump and his family are\")\n",
    "for t in sorted_tuple[0:20]:\n",
    "    print(t[0] + \" --- \" + str(t[1][\"reports\"]), sep=\"\\n\")\n",
    "    urllist = urllist + t[1][\"weburls\"]\n",
    "    \n",
    "print(\"The number of urls we are interested in are: \", len(urllist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieve user comments and find most frequent words\n",
    "#### Analysis - 3\n",
    "\n",
    "- I am intersted in finding user comments on articles of these reporters\n",
    "- The most frequent words that are used by these commenters might give an insight about the general feeling in the population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For all articles in urllist we will fetch comments \n",
    "# Because we have limit on number of requests I am storing the comments data in data folder.\n",
    "\n",
    "outputdir = current_dir + \"/comments/\"\n",
    "createdir(outputdir)\n",
    "def fetchCommentsData():\n",
    "    i = 0\n",
    "    for url in urllist:\n",
    "        resp = requests.get(\"http://api.nytimes.com/svc/community/v3/user-content/url.json?url=\"+ url + \"&\" + api_key)\n",
    "        if resp.status_code is 200:\n",
    "            comments = resp.json()[\"results\"][\"comments\"]\n",
    "            if len(comments) > 0:\n",
    "                i = i + 1           \n",
    "                outputpath = outputdir + \"UsComments\" + str(i) + \".json\"\n",
    "                with open(outputpath, \"w\") as jf:\n",
    "                    json.dump(resp.json(), jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fetchCommentsData() Call this function to get the comments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS # Better collection than NLTK stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "txt_comment = \"\"\n",
    "json_files = glob.glob(outputdir +'/*.json')\n",
    "for file in json_files:\n",
    "    with open(file) as jsonfile:\n",
    "        json_data = json.load(jsonfile)\n",
    "        for i in range(0,len(json_data[\"results\"][\"comments\"])):\n",
    "            comment = json_data[\"results\"][\"comments\"][i]\n",
    "            allwords = word_tokenize(comment[\"commentBody\"])\n",
    "            sents =  [word.lower() for word in allwords if word.lower() not in ENGLISH_STOP_WORDS and word.isalpha()]\n",
    "            txt_comment = txt_comment + ' '.join(sents)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Most common words in comments for these articles\n",
      "('trump', 4141)\n",
      "('people', 1350)\n",
      "('like', 998)\n",
      "('just', 854)\n",
      "('clinton', 817)\n",
      "('republican', 781)\n",
      "('party', 753)\n",
      "('does', 750)\n",
      "('president', 725)\n",
      "('vote', 696)\n",
      "('country', 668)\n",
      "('hillary', 654)\n",
      "('donald', 594)\n",
      "('election', 572)\n",
      "('time', 546)\n",
      "('think', 514)\n",
      "('make', 506)\n",
      "('did', 490)\n",
      "('voters', 484)\n",
      "('republicans', 473)\n"
     ]
    }
   ],
   "source": [
    "#Most common words in Email Subjects for These Employees\n",
    "from nltk.probability import FreqDist\n",
    "fdist1 = FreqDist(word_tokenize(txt_comment))\n",
    "print(\"20 Most common words in comments for these articles\")\n",
    "print(*fdist1.most_common(20), sep=\"\\n\")\n",
    "words = [t[0] for t in fdist1.most_common(50) if t[1] > 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordCloud representation of comment words\n"
     ]
    }
   ],
   "source": [
    "import wordcloud\n",
    "#WordCloud representation of comment words\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "wc = wordcloud.WordCloud(width=800, \n",
    "                         height=600, \n",
    "                         max_words=200,\n",
    "                         stopwords=ENGLISH_STOP_WORDS).generate(txt_comment)\n",
    "ax.imshow(wc)\n",
    "ax.axis(\"off\")\n",
    "fig.savefig(\"Q2img/articlecloud.png\")\n",
    "print(\"WordCloud representation of comment words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WordCloud representation of comment words](Q2img/articlecloud.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bigram words to have a sense of what people had said in comments\n",
    "from nltk import bigrams\n",
    "bigwords = [bg for bg in set(bigrams(word_tokenize(txt_comment))) if bg[0] in words and bg[1] in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('good', 'america')\n",
      "('like', 'clinton')\n",
      "('people', 'donald')\n",
      "('media', 'better')\n",
      "('support', 'world')\n",
      "('years', 'vote')\n",
      "('president', 'political')\n",
      "('hillary', 'clinton')\n",
      "('voters', 'election')\n",
      "('voters', 'just')\n",
      "('people', 'time')\n",
      "('did', 'think')\n",
      "('gop', 'vote')\n",
      "('think', 'just')\n",
      "('really', 'want')\n",
      "('years', 'think')\n",
      "('republican', 'going')\n",
      "('state', 'going')\n",
      "('republican', 'hillary')\n",
      "('media', 'republicans')\n"
     ]
    }
   ],
   "source": [
    "print(*bigwords[0:20], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These words in comments clearly suggest how charged up people were with election."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
